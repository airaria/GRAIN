{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config_file ='teacher_models/config.json'\n",
    "# specify your pruned model\n",
    "ckpt_file='pruned_models/pd-sst2-05/lr3e20_s_bs32_0.4_pf1_IS0.998_Reg3e-1_E192/gs42080.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "import torch\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION']='python'\n",
    "\n",
    "from modeling_prunebert import BertModel as PrunedBertModel\n",
    "from modeling_prunebert import BertForSequenceClassification\n",
    "from modeling_prunebert import set_head_cuts\n",
    "from transformers import BertConfig\n",
    "from textpruner import summary,inference_time\n",
    "from textpruner import TransformerPruner\n",
    "from textpruner.extentions.pruner import FineGrainedPruner\n",
    "\n",
    "config = BertConfig.from_json_file(bert_config_file)\n",
    "config.proj_size = 192\n",
    "\n",
    "state_dict = torch.load(ckpt_file,map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore weights\n",
    "state_dict_items=list(state_dict.items())\n",
    "for k,v in state_dict_items:\n",
    "    if k.endswith('_mask'):\n",
    "        state_dict[k[:-5]] = state_dict[k] * state_dict[k[:-5]+'_orig']\n",
    "keys =  [k for k in state_dict.keys() if k.endswith('_orig')]\n",
    "for k in keys:\n",
    "    del state_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(None,config=config,state_dict=state_dict)\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_mask_list = [state_dict[f'bert.encoder.layer.{i}.output.dense.weight_mask'][0] for i in range(12)]\n",
    "ffn_mask = torch.stack(ffn_mask_list)\n",
    "qk_mask_list = [state_dict[f'bert.encoder.layer.{i}.attention.self.query.bias_mask'] for i in range(12)]\n",
    "vo_mask_list = [state_dict[f'bert.encoder.layer.{i}.attention.self.value.bias_mask'] for i in range(12)]\n",
    "qk_head_size_list = [t.reshape(12,64).sum(-1) for t in qk_mask_list]\n",
    "vo_head_size_list = [t.reshape(12,64).sum(-1) for t in vo_mask_list]\n",
    "\n",
    "# make qk_mask and vo_mask consistent\n",
    "def make_qk_vo_consistency(qk_mask_list,vo_mask_list):\n",
    "    new_qk_mask_list = []\n",
    "    new_vo_mask_list = []\n",
    "    assert len(qk_mask_list)==len(vo_mask_list)\n",
    "    for qk_mask, vo_mask in zip(qk_mask_list, vo_mask_list):\n",
    "        if vo_mask.sum()==0: #important for empty MHA\n",
    "            new_qk_mask = []\n",
    "            new_vo_mask = []\n",
    "        else:\n",
    "            new_qk_mask = []\n",
    "            new_vo_mask = []\n",
    "            qk_head_mask = qk_mask.reshape(12,64)\n",
    "            vo_head_mask = vo_mask.reshape(12,64)\n",
    "            for i,(qk_head, vo_head) in enumerate(zip(qk_head_mask, vo_head_mask)):\n",
    "                if vo_head.sum()==0 and qk_head.sum()==0 :\n",
    "                    continue\n",
    "                else:\n",
    "                    new_qk_mask.append(qk_head.clone())\n",
    "                    new_vo_mask.append(vo_head.clone())\n",
    "            new_qk_mask = torch.stack(new_qk_mask)\n",
    "            new_vo_mask = torch.stack(new_vo_mask)\n",
    "        new_qk_mask_list.append(new_qk_mask)\n",
    "        new_vo_mask_list.append(new_vo_mask)\n",
    "    return new_qk_mask_list,new_vo_mask_list\n",
    "\n",
    "consistent_qk_mask_list,consistent_vo_mask_list =  make_qk_vo_consistency(qk_mask_list,vo_mask_list)\n",
    "consistent_qk_head_size_list = [t.reshape(-1,64).sum(-1).int() if isinstance(t,torch.Tensor) else t for t in consistent_qk_mask_list ]\n",
    "consistent_vo_head_size_list = [t.reshape(-1,64).sum(-1).int() if isinstance(t,torch.Tensor) else t for t in consistent_vo_mask_list ]\n",
    "\n",
    "qk_head_cuts_list = [torch.tensor([0]+list(t)).cumsum(-1) for t in consistent_qk_head_size_list]\n",
    "vo_head_cuts_list = [torch.tensor([0]+list(t)).cumsum(-1) for t in consistent_vo_head_size_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_masks(state_dict):\n",
    "    ffn_mask_list = torch.stack([state_dict[f'bert.encoder.layer.{i}.output.dense.weight_mask'][0] for i in range(12)]).int()\n",
    "    qk_mask_list = torch.stack([state_dict[f'bert.encoder.layer.{i}.attention.self.query.bias_mask'] for i in range(12)]).int()\n",
    "    vo_mask_list = torch.stack([state_dict[f'bert.encoder.layer.{i}.attention.self.value.bias_mask'] for i in range(12)]).int()\n",
    "    qk_head_size_list = [t.reshape(12,64).sum(-1) for t in qk_mask_list]\n",
    "    #qk_head_size_list = [t[t>0] for t in qk_head_size_list]\n",
    "    vo_head_size_list = vo_mask_list.reshape(12,12,64).sum(-1)\n",
    "    #vo_head_size_list = [t[t>0] for t in vo_head_size_list]\n",
    "    print(\"=====VO=====\")\n",
    "    for i in range(12):\n",
    "        print(f\"{i}: {[i for i in vo_head_size_list[i].tolist() if i >0]}, {vo_head_size_list[i].sum().item()}, {(vo_head_size_list[i]>0).sum().item()}\")\n",
    "    print(\"Total number of heads:\",(vo_head_size_list>0).sum().item())\n",
    "    print(\"Total number of MHA layer:\",(vo_head_size_list.sum(-1)>0).sum().item())\n",
    "    \n",
    "    print(\"=====FFN=====\")\n",
    "    print(f\"FFN size/12: {ffn_mask_list.sum(-1).tolist()} {(ffn_mask_list).sum().item()/12:.1f}\")\n",
    "    print(\"Total number of FFN layers:\",(ffn_mask_list.sum(-1)>0).sum().item())\n",
    "show_masks(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randint(low=0,high=10000,size=(128,512),device=device)\n",
    "with torch.no_grad():\n",
    "    mean,std = inference_time(model,[inputs])\n",
    "    print(mean,std)\n",
    "    print(summary(model))\n",
    "    original_outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove weights where mask==1\n",
    "pruner = TransformerPruner(model)\n",
    "pruner.prune(ffn_mask=ffn_mask, save_model=False)\n",
    "pruner =FineGrainedPruner(model)\n",
    "pruner.prune(QK_mask_list=qk_mask_list,VO_mask_list=vo_mask_list,save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty FFN layers and empty MHA layers\n",
    "\n",
    "from torch import nn\n",
    "import types\n",
    "def feed_forward_chunk_for_empty_ffn(self, attention_output):\n",
    "        layer_output = self.output(attention_output)\n",
    "        return layer_output\n",
    "\n",
    "def output_forward(self, input_tensor):\n",
    "        return self.LayerNorm(self.dense.bias + input_tensor)\n",
    "\n",
    "def attetion_forward_for_empty_attention(self,\n",
    "                                        hidden_states,\n",
    "                                        attention_mask=None,\n",
    "                                        head_mask=None,\n",
    "                                        encoder_hidden_states=None,\n",
    "                                        encoder_attention_mask=None,\n",
    "                                        past_key_value=None,\n",
    "                                        output_attentions=False):\n",
    "    hidden_states = self.output.LayerNorm(self.output.dense.bias + hidden_states)\n",
    "    return (hidden_states,)\n",
    "\n",
    "def transform(model: nn.Module,always_ffn=False, always_mha=False):\n",
    "    base_model = model.base_model\n",
    "    bert_layers = base_model.encoder.layer\n",
    "    for layer in bert_layers:\n",
    "        output = layer.output\n",
    "        if always_ffn or output.dense.weight.numel()==0: #empty ffn\n",
    "            print(\"replace ffn\")\n",
    "            layer.feed_forward_chunk = types.MethodType(feed_forward_chunk_for_empty_ffn,layer)\n",
    "            layer.output.forward = types.MethodType(output_forward,layer.output)\n",
    "        attention_output = layer.attention.output\n",
    "        if always_mha or attention_output.dense.weight.numel()==0: #empty attention\n",
    "            print(\"replace mha\")\n",
    "            layer.attention.forward = types.MethodType(attetion_forward_for_empty_attention,layer.attention)\n",
    "\n",
    "transform(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_head_cuts(model,qk_head_cuts_list,vo_head_cuts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pruned_outputs = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcuate the discrepency between unpruned and pruned models\n",
    "torch.max((pruned_outputs.logits-original_outputs.logits).abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show model size\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference time\n",
    "\n",
    "inputs = torch.randint(low=0,high=10000,size=(128,512),device=device)\n",
    "with torch.no_grad():\n",
    "    mean,std = inference_time(model,[inputs])\n",
    "\n",
    "print(\"Mean: \", mean)\n",
    "print(\"Std: \", std)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
